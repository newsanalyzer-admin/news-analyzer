# Story 2.1.3: Entity Disambiguation

## Status
Draft

## Story
**As a** system,
**I want to** disambiguate between multiple candidate entities from external knowledge bases,
**so that** I link to the correct external record with high confidence.

## Acceptance Criteria
1. Use entity type as primary disambiguation signal (40% weight)
2. Use name similarity as secondary signal (30% weight)
3. Use context (article text) for additional disambiguation (30% weight)
4. Return confidence score (0.0-1.0) for final match
5. Flag low-confidence matches (<0.7) for manual review
6. Support manual disambiguation override via API
7. Handle ambiguous entities (e.g., "Washington" could be person, place, or organization)
8. Log disambiguation decisions for debugging and improvement

## Tasks / Subtasks
- [ ] Create DisambiguationService class (AC: 1, 2, 3)
  - [ ] Implement type matching scorer
  - [ ] Implement name similarity scorer (Levenshtein/Jaro-Winkler)
  - [ ] Implement context similarity scorer (TF-IDF or simple keyword overlap)
  - [ ] Combine scores with configurable weights
- [ ] Implement confidence thresholding (AC: 4, 5)
  - [ ] Calculate weighted final score
  - [ ] Mark matches below 0.7 as "needs_review"
  - [ ] Return confidence alongside match
- [ ] Add manual override support (AC: 6)
  - [ ] Store manual linking decisions in database
  - [ ] Check for existing manual link before auto-disambiguation
  - [ ] API endpoint to record manual decision
- [ ] Handle ambiguous cases (AC: 7)
  - [ ] Detect highly ambiguous names (multiple types possible)
  - [ ] Require context for ambiguous names
  - [ ] Lower confidence when ambiguity detected
- [ ] Add logging and metrics (AC: 8)
  - [ ] Log each disambiguation decision with inputs and scores
  - [ ] Track disambiguation success rate
  - [ ] Track manual review rate
- [ ] Write unit tests
  - [ ] Test type matching scoring
  - [ ] Test name similarity scoring
  - [ ] Test context similarity scoring
  - [ ] Test combined scoring
  - [ ] Test ambiguous name handling

## Dev Notes

### Relevant Source Tree
```
reasoning-service/
├── app/
│   ├── services/
│   │   ├── disambiguation.py   # NEW - Create this file
│   │   ├── wikidata_client.py  # Story 2.1.1 - provides candidates
│   │   ├── dbpedia_client.py   # Story 2.1.2 - provides candidates
│   │   └── entity_linker.py    # Story 2.1.4 - will use this service
└── tests/
    └── test_disambiguation.py  # NEW - Create this file
```

### Disambiguation Algorithm

```python
def disambiguate(
    entity_text: str,
    entity_type: str,
    candidates: List[Candidate],
    context: Optional[str] = None
) -> DisambiguationResult:
    """
    Score and rank candidates to find best match.

    Weights:
    - Type match: 0.4
    - Name similarity: 0.3
    - Context similarity: 0.3 (or 0.0 if no context)
    """

    for candidate in candidates:
        type_score = score_type_match(entity_type, candidate.types)
        name_score = score_name_similarity(entity_text, candidate.label)
        context_score = score_context(context, candidate.description) if context else 0.5

        # Weighted combination
        if context:
            final_score = (type_score * 0.4) + (name_score * 0.3) + (context_score * 0.3)
        else:
            # Without context, reweight type and name
            final_score = (type_score * 0.5) + (name_score * 0.5)

        candidate.score = final_score

    best = max(candidates, key=lambda c: c.score)

    return DisambiguationResult(
        match=best if best.score >= 0.7 else None,
        confidence=best.score,
        needs_review=best.score < 0.7,
        all_candidates=sorted(candidates, key=lambda c: -c.score)
    )
```

### Type Matching Logic

| Entity Type | Wikidata Match (score 1.0) | Partial Match (score 0.5) |
|-------------|---------------------------|---------------------------|
| person | Q5 (human) | Q215627 (person) |
| government_org | Q327333 (gov agency) | Q43229 (organization) |
| organization | Q43229 (organization) | Q4830453 (business) |
| location | Q515 (city), Q6256 (country) | Q82794 (region) |

### Name Similarity Algorithms

Use `rapidfuzz` library for efficient string matching:
```python
from rapidfuzz import fuzz

def score_name_similarity(name1: str, name2: str) -> float:
    # Normalize names
    n1 = normalize(name1)  # lowercase, remove punctuation
    n2 = normalize(name2)

    # Check for acronym match
    if is_acronym(n1) and matches_acronym(n1, n2):
        return 0.95  # High score for acronym match

    # Use token set ratio for partial matches
    return fuzz.token_set_ratio(n1, n2) / 100.0
```

### Context Similarity (Simple Version)

For MVP, use keyword overlap:
```python
def score_context(context: str, description: str) -> float:
    if not description:
        return 0.5  # Neutral score

    context_words = set(context.lower().split())
    desc_words = set(description.lower().split())

    # Remove stop words
    context_words -= STOP_WORDS
    desc_words -= STOP_WORDS

    if not desc_words:
        return 0.5

    overlap = len(context_words & desc_words)
    return min(1.0, overlap / 10.0)  # Cap at 1.0
```

### Dependencies to Add
```
# requirements.txt additions
rapidfuzz>=3.0.0  # Fast string matching
```

### Ambiguous Name Detection

Known ambiguous patterns:
- Single common names: "Washington", "Johnson", "Clinton"
- Acronyms with multiple meanings: "DOJ", "SEC"
- Generic terms: "Senate", "Congress" (which country?)

```python
AMBIGUOUS_NAMES = {
    "washington": ["person", "location", "government_org"],
    "johnson": ["person", "organization"],
    "clinton": ["person", "location"],
}

def is_ambiguous(name: str) -> bool:
    return normalize(name) in AMBIGUOUS_NAMES
```

## Testing

### Test File Location
`reasoning-service/tests/test_disambiguation.py`

### Testing Standards
- Use pytest framework
- Test edge cases thoroughly (ambiguous names, missing context)
- Minimum 95% coverage for this critical module

### Test Cases
```python
def test_type_match_scoring_exact():
    """Test exact type match returns 1.0"""

def test_type_match_scoring_partial():
    """Test partial type match returns 0.5"""

def test_name_similarity_exact():
    """Test exact name match returns 1.0"""

def test_name_similarity_acronym():
    """Test 'EPA' matches 'Environmental Protection Agency'"""

def test_context_improves_disambiguation():
    """Test context helps distinguish between candidates"""

def test_low_confidence_flagged():
    """Test matches below 0.7 are flagged for review"""

def test_ambiguous_name_requires_context():
    """Test 'Washington' without context has lower confidence"""

def test_manual_override_respected():
    """Test manual linking decision overrides auto-disambiguation"""

def test_combined_scoring():
    """Test final score is weighted combination"""
```

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-25 | 1.0 | Initial story creation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used
_To be filled by dev agent_

### Debug Log References
_To be filled by dev agent_

### Completion Notes List
_To be filled by dev agent_

### File List
_To be filled by dev agent_

## QA Results
_To be filled by QA agent_
